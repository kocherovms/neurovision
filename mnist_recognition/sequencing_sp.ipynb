{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c8bb0-a8e9-4fec-adc0-8619236e2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEQUENCING SPARE PARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034889a8-1439-4433-80e3-28e00f14b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_RECEPTIVE_FIELD_SIZE = 8\n",
    "SENSOR_RECEPTIVE_FIELD_SQUARE = SENSOR_RECEPTIVE_FIELD_SIZE ** 2\n",
    "\n",
    "SENSORS_COUNT = config.sensors_count\n",
    "SENSORS_COUNT_ROOT = int(np.sqrt(SENSORS_COUNT))\n",
    "assert SENSORS_COUNT_ROOT ** 2 == SENSORS_COUNT\n",
    "assert SENSORS_COUNT_ROOT > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ff2ef-d3f8-4d78-978a-e611bce73739",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_STATES_INFO = defaultdict(list)\n",
    "SENSOR_STATES_IMG = []\n",
    "sz = SENSOR_RECEPTIVE_FIELD_SIZE\n",
    "captions = []\n",
    "\n",
    "assert config.encoding_type == 'sequence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de2dd1-932b-4418-8584-8ab6ee1a7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_step = 10 # degrees\n",
    "angles = list(range(0, 360, angle_step)) + [45 * 1, 45 * 3, 45 * 5, 45 * 7]\n",
    "angles = sorted(angles)\n",
    "# sweeps = [50, 90, 130, 180, 230, 270, 310] # for receptive field size = 8 more finer sweeps make little sense \n",
    "sweeps = [90, 130, 180, 230, 270] # for receptive field size = 8 more finer sweeps make little sense \n",
    "\n",
    "for angle in angles:\n",
    "    # for sweep in range(angle_step, 180 + angle_step, angle_step):\n",
    "    for sweep in sweeps:\n",
    "        canvas = Image.new('L', (sz, sz))\n",
    "        draw = ImageDraw.Draw(canvas)\n",
    "        draw.rectangle([0, 0, sz, sz], fill=127, outline=127)\n",
    "        piesclice_angle1 = angle + sweep // 2\n",
    "        piesclice_angle2 = angle - sweep // 2\n",
    "        draw.pieslice([-sz, -sz, sz * 2 - 1, sz * 2 - 1], start=piesclice_angle1, end=piesclice_angle2, fill=255, outline=255, width=0)\n",
    "        captions.append(f'ang={angle}, swp={sweep} ')\n",
    "        SENSOR_STATES_IMG.append(canvas)\n",
    "        SENSOR_STATES_INFO['normal'].append(angle)\n",
    "        SENSOR_STATES_INFO['sweep'].append(sweep)\n",
    "        SENSOR_STATES_INFO['normal_vec'].append(np.exp((1j) * np.radians(angle)))\n",
    "        SENSOR_STATES_INFO['sweep1_vec'].append(np.exp((1j) * np.radians(piesclice_angle1)))\n",
    "        SENSOR_STATES_INFO['sweep2_vec'].append(np.exp((1j) * np.radians(piesclice_angle2)))\n",
    "\n",
    "SENSOR_STATES_INFO = pd.DataFrame(SENSOR_STATES_INFO)\n",
    "assert len(SENSOR_STATES_INFO) == len(SENSOR_STATES_IMG)\n",
    "# n_to_show = len(sweeps) * 4\n",
    "# images_to_show = list(map(lambda x: lay_grid(x.resize((80, 80)), 8), SENSOR_STATES_IMG[:n_to_show]))\n",
    "# display_images_grid(images_to_show, captions=captions[:n_to_show], col_count=len(sweeps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad353d-0575-4260-8f03-1791d3671cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn grayscale images of sensor states to numpy arrays with weights which balance positive and negative areas\n",
    "SENSOR_STATES = np.array(list(map(np.array, SENSOR_STATES_IMG))).astype(float)\n",
    "SENSOR_STATES = SENSOR_STATES.reshape(-1, SENSOR_RECEPTIVE_FIELD_SIZE ** 2)\n",
    "SENSOR_STATES[SENSOR_STATES == 255] = 1\n",
    "SENSOR_STATES[SENSOR_STATES == 127] = -1\n",
    "assert set(np.unique(SENSOR_STATES)) == set([-1, +1])\n",
    "\n",
    "SENSOR_STATES_COUNT = SENSOR_STATES.shape[0]\n",
    "# SENSOR_STATES.shape, np.unique_counts(SENSOR_STATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0366d29-2c22-4f5a-8b6c-401b41b7daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_STATE_AREAS_POS = (SENSOR_STATES == 1).sum(axis=1) \n",
    "SENSOR_STATE_AREAS_NEG = (SENSOR_STATES == -1).sum(axis=1)\n",
    "assert np.all(SENSOR_STATE_AREAS_POS > 0)\n",
    "assert np.all(SENSOR_STATE_AREAS_NEG > 0)\n",
    "# SENSOR_STATE_AREAS_POS.shape, SENSOR_STATE_AREAS_POS.mean(), SENSOR_STATE_AREAS_POS.std(), \\\n",
    "# SENSOR_STATE_AREAS_NEG.shape, SENSOR_STATE_AREAS_NEG.mean(), SENSOR_STATE_AREAS_NEG.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8f0c3-a843-44cd-806c-cef481bbfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_STATES_POS = xp_array_to_gpu_copy(SENSOR_STATES)\n",
    "SENSOR_STATES_NEG = xp_array_to_gpu_copy(SENSOR_STATES)\n",
    "SENSOR_STATES_POS[SENSOR_STATES_POS < 0] = 0\n",
    "SENSOR_STATES_NEG[SENSOR_STATES_NEG > 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c07b07-a2a3-41bd-8a1c-044164a962eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorLayoutMaker(object):\n",
    "    def __call__(self, sensor_index):\n",
    "        return None\n",
    "\n",
    "class SensorLayoutMaker_Grid(SensorLayoutMaker):\n",
    "    def __init__(self):\n",
    "        self.s = SENSORS_COUNT_ROOT\n",
    "        self.field_size = SENSOR_RECEPTIVE_FIELD_SIZE\n",
    "        space_for_sensors = self.s * self.field_size \n",
    "        self.dist_between_sensors = (config.sample_size - space_for_sensors) / (self.s - 1)\n",
    "        \n",
    "    def __call__(self, sensor_index):\n",
    "        x = int((sensor_index % self.s) * (self.field_size + self.dist_between_sensors))\n",
    "        y = int((sensor_index // self.s) * (self.field_size + self.dist_between_sensors))\n",
    "        # adjust to fit into boundaries\n",
    "        x -= max(0, (x + self.field_size) - config.sample_size) \n",
    "        y -= max(0, (y + self.field_size) - config.sample_size)\n",
    "        return x, y, self.field_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82fd8b-3f37-4c61-96af-613d39a6cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_LAYOUT_MAKER = None\n",
    "    \n",
    "match config.retina_layout:\n",
    "    case 'grid': \n",
    "        SENSOR_LAYOUT_MAKER = SensorLayoutMaker_Grid()\n",
    "    case _:\n",
    "        assert False, config.retina_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd7b82-eb8b-46ab-8406-d0b0dad53cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not SENSOR_LAYOUT_MAKER is None\n",
    "SENSOR_INSTANCES_INFO = pd.DataFrame(columns=['x_offset', 'y_offset', 'size', 'bounds_rect', 'x_center', 'y_center', 'radius'])\n",
    "\n",
    "for i in range(SENSORS_COUNT):\n",
    "    x_offset, y_offset, field_size = SENSOR_LAYOUT_MAKER(i)\n",
    "    assert x_offset >= 0 and x_offset <= config.sample_size - field_size, (i, x_offset, field_size)\n",
    "    assert y_offset >= 0 and y_offset <= config.sample_size - field_size, (i, y_offset, field_size)\n",
    "    SENSOR_INSTANCES_INFO.loc[len(SENSOR_INSTANCES_INFO)] = [\n",
    "        x_offset,\n",
    "        y_offset,\n",
    "        field_size,\n",
    "        [x_offset, y_offset, x_offset + field_size - 1, y_offset + field_size - 1],\n",
    "        x_offset + field_size // 2,\n",
    "        y_offset + field_size // 2,\n",
    "        field_size // 2\n",
    "    ]\n",
    "\n",
    "# SENSOR_INSTANCES_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09353c08-2463-4158-ac8e-54bbe3a3bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_PATCH_COORDS = []\n",
    "    \n",
    "for row in SENSOR_INSTANCES_INFO.itertuples():\n",
    "    x_offset = row.x_offset\n",
    "    y_offset = row.y_offset\n",
    "    size = row.size\n",
    "\n",
    "    for i in range(size):\n",
    "        y = (y_offset + i) * config.sample_size\n",
    "        stride_coords = np.arange(y + x_offset, y + x_offset + size)\n",
    "        SENSOR_PATCH_COORDS.append(stride_coords)\n",
    "\n",
    "SENSOR_PATCH_COORDS = np.array(SENSOR_PATCH_COORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e125b8-dc84-416b-9232-9324574bcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_patches_for_sensor_instances(images_matrix):\n",
    "    assert images_matrix.ndim == 2, images_matrix.ndim\n",
    "    assert images_matrix.shape[1] == config.sample_size ** 2, images_matrix.shape[1]\n",
    "    images_count = images_matrix.shape[0]\n",
    "    # Result is 3d-tensor: 1 dim - image, 2 dim - patch no, 3 dim - patch itself (pixels)\n",
    "    return np.take(images_matrix, SENSOR_PATCH_COORDS, axis=1).reshape(images_count, -1, SENSOR_RECEPTIVE_FIELD_SQUARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4655cc-5952-4064-816e-08dacb6312df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns matrix of sense_vectors (one row = sense_vector for each image in images_matrix)\n",
    "# Each elem in sense_vector describes sensor state of each sensor instance (-1 if sensor instance is not activated)\n",
    "def sense_images(images_matrix):\n",
    "    MINIMAL_ILLUMINATION_ABS_LEVEL = 255 * 0.1\n",
    "    MINIMAL_ILLUMINATION_DIFF_DB = 2 # dB\n",
    "    MINIMAL_ILLUMINATION_DIFF_RATIO = pow(10, MINIMAL_ILLUMINATION_DIFF_DB/20)\n",
    "    MINIMAL_PLUS_MINUS_DISTR_REL_DIFF = 0.9\n",
    "    \n",
    "    assert images_matrix.ndim == 2\n",
    "    images_count = images_matrix.shape[0]\n",
    "    images_patches = get_image_patches_for_sensor_instances(images_matrix) # array of matrices: image no -> matrix with patches as row vectors\n",
    "    \n",
    "    mean_luminiscene_in_patches = images_patches.mean(axis=2) # matrix where each row = avg of luminisnece in each patch of an image\n",
    "    mean_images_patches = images_patches.reshape(-1, SENSOR_RECEPTIVE_FIELD_SQUARE).T - mean_luminiscene_in_patches.ravel()\n",
    "    mean_images_patches = mean_images_patches.T.reshape(images_count, -1, SENSOR_RECEPTIVE_FIELD_SQUARE).transpose(0, 2, 1)\n",
    "    # mean_images_patches is an array of matrices: image no -> matrix with mean-centered patches as column vectors\n",
    "    \n",
    "    images_patches = images_patches.transpose(0, 2, 1) # array of matrices: image no -> matrix with patches as column vectors\n",
    "    assert images_patches.shape == mean_images_patches.shape\n",
    "    \n",
    "    abs_illum_level_pos = (SENSOR_STATES_POS @ images_patches).transpose(0, 2, 1)\n",
    "    abs_illum_level_neg = (SENSOR_STATES_NEG @ images_patches).transpose(0, 2, 1)\n",
    "    rel_illum_level_pos = (SENSOR_STATES_POS @ xp.where(mean_images_patches > 0, +1, 0)).transpose(0, 2, 1) # for +/- tally\n",
    "    rel_illum_level_neg = (SENSOR_STATES_NEG @ xp.where(mean_images_patches < 0, -1, 0)).transpose(0, 2, 1)  # for +/- tally\n",
    "    \n",
    "    assert abs_illum_level_pos.shape == (images_count, SENSORS_COUNT, SENSOR_STATES_COUNT), abs_illum_level_pos.shape\n",
    "    assert abs_illum_level_neg.shape == abs_illum_level_pos.shape\n",
    "    assert rel_illum_level_pos.shape == abs_illum_level_pos.shape\n",
    "    assert rel_illum_level_neg.shape == abs_illum_level_pos.shape\n",
    "    \n",
    "    sense_vectors = []\n",
    "    # For each patch in image we detect likelihood of activation of sensor in one of its (sensor's) state.\n",
    "    # If we have some state (say some angle and some sweep), what is condition that given patch corresponds to this state?\n",
    "    # Of course patch must be somewhat illuminated (regulated by SENSORS_POS) - this is to ensure that we don't get fooled by situation\n",
    "    # when e.g. negative part is not illuminated (dot prod is 0) while positive part is illuminated very, very week (say dot prod is 5). Looks\n",
    "    # like this is the reason why we don't hear very low frequencies or don't percieve low light - becase ratio get oversaturaed very quickly.\n",
    "    # But the key point is that sum of pixels luminiscence can meet averaged criterion only when bright pixels are concentared in area covered by SENSORS_POS\n",
    "    # and dark pixels are concentrated in area covered by SENSORS_NEG. Otherwise there will disbalance.\n",
    "    # To detect this condition we can look at distribution of mean-centered patches: +pixels must reside in positive part, -pixels must reside in negative part.\n",
    "    # To restassure that +/- distribution of pixels actually corresponds to detectable diff in illumination we also need to compare\n",
    "    # absolute illumination on positive and negative part\n",
    "    for ailp_i, ailn_i, rilp_i, riln_i in zip(abs_illum_level_pos, abs_illum_level_neg, rel_illum_level_pos, rel_illum_level_neg): # Per image cycle\n",
    "        # ailp_i and ailn_i both are matrices N(patches count) x M(count of sensor states)\n",
    "        shape_save = ailp_i.shape\n",
    "    \n",
    "        # MAC {0, 1}\n",
    "        # Illumination level on positive area must exceed lower threshold (sensor must be somewhat illuminated)\n",
    "        # abs_illumination_scores = matrix of N(patches count), columns - 0/1 if patch is minimally illuminated according to given sensor's state\n",
    "        abs_illumination_scores = ailp_i > MINIMAL_ILLUMINATION_ABS_LEVEL * SENSOR_STATE_AREAS_POS\n",
    "    \n",
    "        # MAC {0, 1}\n",
    "        # Illumination level on positive part must distinguishably exceed illumination level of negative part\n",
    "        ailn_i += 1e-6 # to get rid of possible division by zero errors\n",
    "        ailp_i = ailp_i / SENSOR_STATE_AREAS_POS\n",
    "        ailn_i = ailn_i / SENSOR_STATE_AREAS_NEG\n",
    "        diff_abs_illumination_scores = xp.abs(ailp_i.ravel() / ailn_i.ravel())\n",
    "        diff_abs_illumination_scores = diff_abs_illumination_scores >= MINIMAL_ILLUMINATION_DIFF_RATIO\n",
    "        # diff_abs_illumination_scores = matrix of N(patches count), columns - 0/1 if patch is illuminated more on positive part than on negative\n",
    "        diff_abs_illumination_scores = diff_abs_illumination_scores.reshape(shape_save)\n",
    "    \n",
    "        # Score {0, [MINIMAL_PLUS_MINUS_DISTR_REL_DIFF...1]}\n",
    "        # Tally concentration of +/- pixels on positive/negative side and compare against theirs areas. \n",
    "        # For sensors with proper distribution ratio will be around 1 for each of the scores.\n",
    "        # *concentration_scores = matrix of N(patches count), columns - [0...1] how well pixels in patch are distributed according to given sensor's state\n",
    "        pluses_concentration_scores = rilp_i / SENSOR_STATE_AREAS_POS\n",
    "        # assert xp.all((pluses_concentration_scores >= 0) & (pluses_concentration_scores <= 1)) # comment for production (to speed up)\n",
    "        mask = (pluses_concentration_scores < MINIMAL_PLUS_MINUS_DISTR_REL_DIFF)\n",
    "        pluses_concentration_scores = xp.where(mask, 0, pluses_concentration_scores)\n",
    "    \n",
    "        minuses_concentration_scores = riln_i / SENSOR_STATE_AREAS_NEG\n",
    "        # assert xp.all((minuses_concentration_scores >= 0) & (minuses_concentration_scores <= 1)) # comment for production (to speed up)\n",
    "        mask = (minuses_concentration_scores < MINIMAL_PLUS_MINUS_DISTR_REL_DIFF)\n",
    "        minuses_concentration_scores = xp.where(mask, 0, minuses_concentration_scores)\n",
    "        \n",
    "        # sense_matrix = matrix of N(patches count), columns - 0/1 if all three conditions meet for given sensor state\n",
    "        sense_matrix = (abs_illumination_scores.astype(float) + diff_abs_illumination_scores.astype(float) + pluses_concentration_scores + minuses_concentration_scores)\n",
    "        min_score = 1 + 1 + (MINIMAL_PLUS_MINUS_DISTR_REL_DIFF * 2)\n",
    "        sense_vector = xp.where(xp.any(sense_matrix > min_score, axis=1), xp.argmax(sense_matrix, axis=1), -1)\n",
    "        sense_vectors.append(sense_vector)\n",
    "    \n",
    "    return xp.vstack(sense_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51587889-5978-49a4-8cde-0bd04c81fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SensorInstance = namedtuple('SensorInstance', ['Index', 'sensor_instance_ind', 'sensor_state_ind', 'normal', 'sweep', 'radius', \n",
    "                                               'x_center', 'y_center', 'x_center_orig', 'y_center_orig', 'altitude', 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e7c0d-54c0-4d9e-920c-0b4dc6047ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (altitude_map, df_sensor_instances) for sense_vector\n",
    "def parse_sense_vector(sense_vector):\n",
    "    ALTITUDE_EXTEND_BY = 5 # number of new intermediate cells for each side of 2x2 rect, e.g. 3x3 becomes 13x13 if altitude_extend_by is 5\n",
    "    MAX_LANDING_STEPS = 1000\n",
    "    MAX_ALTITUDE_THRESHOLD = 2\n",
    "    \n",
    "    assert sense_vector.shape[0] == len(SENSOR_INSTANCES_INFO)\n",
    "    sensor_instance_inds = np.argwhere(sense_vector != -1).ravel()\n",
    "    sensor_state_inds = np.take(sense_vector, sensor_instance_inds)\n",
    "    assert sensor_instance_inds.shape[0] == sensor_state_inds.shape[0]\n",
    "    df_sensor_instances_raw = pd.DataFrame({'sensor_instance_ind': sensor_instance_inds, 'sensor_state_ind': sensor_state_inds})\n",
    "    df_sensor_instances_raw = df_sensor_instances_raw.merge(SENSOR_STATES_INFO, how='left', left_on='sensor_state_ind', right_index=True)\n",
    "    df_sensor_instances_raw = df_sensor_instances_raw.merge(SENSOR_INSTANCES_INFO, how='left', left_on='sensor_instance_ind', right_index=True)\n",
    "\n",
    "    # Lo-resolution altitude map\n",
    "    altitude_map_lores = np.zeros((config.sample_size, config.sample_size))\n",
    "\n",
    "    for si in df_sensor_instances_raw.itertuples():\n",
    "        sensor_state = SENSOR_STATES[si.sensor_state_ind]\n",
    "        size = si.size\n",
    "        altitude_map_lores[si.y_offset:si.y_offset + size, si.x_offset:si.x_offset + size] += sensor_state.reshape(size, size)\n",
    "    \n",
    "    altitude_map_lores_mask = np.full_like(altitude_map_lores, np.min(altitude_map_lores), dtype='i')\n",
    "    \n",
    "    for si in df_sensor_instances_raw.itertuples():\n",
    "        altitude_map_lores_mask[si.y_offset:si.y_offset + size, si.x_offset:si.x_offset + size] = 0\n",
    "    \n",
    "    altitude_map_lores += altitude_map_lores_mask\n",
    "    altitude_map_lores = altitude_map_lores.astype('i')\n",
    "\n",
    "    # Enhance lo-resolution altitude map by interpolating. This is to develop low altitude areas which are \n",
    "    # hidden beyond patches like [[-4, -5], [3, 4]]\n",
    "    altitude_lores_to_hires_translate_factor = (ALTITUDE_EXTEND_BY + 1)\n",
    "    altitude_map_hires_size = (config.sample_size - 1) * altitude_lores_to_hires_translate_factor + 1\n",
    "    altitude_map_hires = np.zeros((altitude_map_hires_size, altitude_map_hires_size), dtype='f')\n",
    "    patch_hires_shape = (2 + ALTITUDE_EXTEND_BY, 2 + ALTITUDE_EXTEND_BY)\n",
    "    ii, jj = np.mgrid[0:patch_hires_shape[0], 0:patch_hires_shape[1]]\n",
    "    patch_hires_indices = np.array(list(zip(ii.ravel(), jj.ravel())))\n",
    "    \n",
    "    for i in range(altitude_map_lores.shape[0] - 1):\n",
    "        for j in range(altitude_map_lores.shape[1] - 1):\n",
    "            patch = altitude_map_lores[i:i+2, j:j+2].astype(float)\n",
    "    \n",
    "            if patch[0,0] == patch[0,1] and patch[0,1] == patch[1,0] and patch[1,0] == patch[1,1]:\n",
    "                patch_hires = patch[0,0]\n",
    "            else:\n",
    "                interp = RegularGridInterpolator(([0, patch_hires_shape[0] - 1], [0, patch_hires_shape[1] - 1]), patch)\n",
    "                patch_hires = interp(patch_hires_indices).reshape(patch_hires_shape)\n",
    "                \n",
    "            i_hires = i * altitude_lores_to_hires_translate_factor\n",
    "            j_hires = j * altitude_lores_to_hires_translate_factor\n",
    "            altitude_map_hires[i_hires:i_hires+patch_hires_shape[0], j_hires:j_hires+patch_hires_shape[1]] = patch_hires\n",
    "\n",
    "    # Reposition sensor instances so they occur in a valley with low absolute altitude. Some of the sensor instances will get discarded\n",
    "    landed_centers = defaultdict(list)\n",
    "\n",
    "    for si in df_sensor_instances_raw.itertuples():\n",
    "        x_center_land = si.x_center * altitude_lores_to_hires_translate_factor\n",
    "        y_center_land = si.y_center * altitude_lores_to_hires_translate_factor\n",
    "        altitude = altitude_map_hires[y_center_land, x_center_land]\n",
    "    \n",
    "        if np.abs(altitude) > 1:\n",
    "            travel_point = complex(x_center_land, y_center_land)\n",
    "            direction = -1 if altitude < 0 else +1\n",
    "            \n",
    "            for i in range(MAX_LANDING_STEPS):\n",
    "                travel_point += direction * si.normal_vec\n",
    "                new_x_center_land = np.round(travel_point.real, decimals=0).astype(int)\n",
    "                new_y_center_land = np.round(travel_point.imag, decimals=0).astype(int)\n",
    "                new_altitude = np.abs(altitude_map_hires[new_y_center_land, new_x_center_land])\n",
    "    \n",
    "                if new_altitude <= altitude:\n",
    "                    altitude = new_altitude\n",
    "                    x_center_land = new_x_center_land\n",
    "                    y_center_land = new_y_center_land\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                assert False\n",
    "    \n",
    "        landed_centers['x_center_new'].append(x_center_land)\n",
    "        landed_centers['y_center_new'].append(y_center_land)\n",
    "        landed_centers['x_center_orig'].append(int(x_center_land / altitude_lores_to_hires_translate_factor))\n",
    "        landed_centers['y_center_orig'].append(int(y_center_land / altitude_lores_to_hires_translate_factor))\n",
    "        landed_centers['altitude_new'].append(altitude)\n",
    "    \n",
    "    df_sensor_instances = df_sensor_instances_raw.copy()\n",
    "    \n",
    "    for col_name, values in landed_centers.items():\n",
    "        df_sensor_instances[col_name] = values\n",
    "    \n",
    "    df_sensor_instances.drop(['x_center', 'y_center', 'x_offset', 'y_offset', 'bounds_rect', 'normal_vec', 'sweep1_vec', 'sweep2_vec', 'size'], axis=1, inplace=True)\n",
    "    df_sensor_instances.rename({'x_center_new': 'x_center', 'y_center_new': 'y_center', 'altitude_new': 'altitude'}, axis=1, inplace=True, errors='raise')\n",
    "    df_sensor_instances = df_sensor_instances.loc[np.abs(df_sensor_instances.altitude) < MAX_ALTITUDE_THRESHOLD]\n",
    "    df_sensor_instances['x'] = df_sensor_instances.x_center.astype(str) + ',' + df_sensor_instances.y_center.astype(str)\n",
    "    df_sensor_instances.drop_duplicates(subset=['x'], inplace=True)\n",
    "    return (altitude_map_hires, df_sensor_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8eb30-70a8-44a7-852b-eb09006d7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_raw_sequences(sensor_instance_ind_dict, altitude_map):\n",
    "    sensor_instance_map = dict(map(lambda si: ((si.y_center, si.x_center), si), sensor_instance_ind_dict.values()))\n",
    "    unsorted_sensor_instance_inds = set(map(lambda si: si.Index, sensor_instance_ind_dict.values()))\n",
    "    raw_sequences = {} # start_sensor_inst_index -> sensor_inst_chains\n",
    "    \n",
    "    while unsorted_sensor_instance_inds:\n",
    "        # choose random sensor instance to start from\n",
    "        start_sensor_inst_index = unsorted_sensor_instance_inds.pop()\n",
    "        start_sensor_inst = sensor_instance_ind_dict[start_sensor_inst_index]\n",
    "        \n",
    "        wave_front_que = deque([(int(start_sensor_inst.y_center), int(start_sensor_inst.x_center))])\n",
    "        wave_front_map = np.full_like(altitude_map, -1, dtype='i')\n",
    "        wave_front_map[wave_front_que[0]] = start_sensor_inst.Index\n",
    "        wave_front_touched = set([wave_front_que[0]])\n",
    "        \n",
    "        sensor_inst_chains = {} # key - last element in chain of sensor instances, value - chain itself\n",
    "        \n",
    "        # LOG(f'Starting countour from {start_sensor_inst_index} (of {len(unsorted_sensor_instance_inds)})')\n",
    "        \n",
    "        while wave_front_que:\n",
    "            # Visit point on top of the wave_front\n",
    "            wave_front_point = wave_front_que.popleft()\n",
    "            wave_front_sensor_inst_ind = wave_front_map[wave_front_point]\n",
    "            wave_front_map[wave_front_point] = -1\n",
    "            assert wave_front_sensor_inst_ind > -1\n",
    "            # LOG(f'Visiting {wave_front_point} (si={wave_front_sensor_inst_ind}) (of {len(wave_front_que)})')\n",
    "        \n",
    "            si = sensor_instance_map.get(wave_front_point, None)\n",
    "        \n",
    "            if not si is None and si.Index != wave_front_sensor_inst_ind:\n",
    "                # New sensor instance discovered! Link with the previous one, ...\n",
    "                sensor_inst_chain = sensor_inst_chains.pop(wave_front_sensor_inst_ind, [wave_front_sensor_inst_ind])\n",
    "                sensor_inst_chain.append(si.Index)\n",
    "                assert not si.Index in sensor_inst_chains, f'{si.Index} in sensor_inst_chains'\n",
    "                sensor_inst_chains[si.Index] = sensor_inst_chain\n",
    "                \n",
    "                # LOG(f'New si encountered: {si.Index}, linked after {wave_front_sensor_inst_ind}')\n",
    "                wave_front_sensor_inst_ind = si.Index\n",
    "                # ... and adjust adjacent wave front points to account for new sensor instance\n",
    "                # (adjacency criteria enables coexisting of separate (disjoint) wave fronts on a map)\n",
    "                adjust_stack = [wave_front_point]\n",
    "                adjust_touched = set([wave_front_point])\n",
    "        \n",
    "                while adjust_stack:\n",
    "                    adjust_point = adjust_stack.pop()\n",
    "        \n",
    "                    for rel_ij in [[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 1], [1, -1], [1, 0], [1, 1]]:\n",
    "                        ij = int(adjust_point[0] + rel_ij[0]), int(adjust_point[1] + rel_ij[1])\n",
    "                \n",
    "                        if ij[0] < 0 or ij[1] < 0 or ij[0] >= wave_front_map.shape[0] or ij[1] >= wave_front_map.shape[1]:\n",
    "                            continue\n",
    "                        elif wave_front_map[ij] == -1:\n",
    "                            continue\n",
    "                        elif ij in adjust_touched:\n",
    "                            continue\n",
    "        \n",
    "                        # LOG(f'Adjusting {ij} (si={wave_front_map[ij]}) to si={si.Index}')\n",
    "                        wave_front_map[ij] = si.Index\n",
    "                        adjust_stack.append(ij)\n",
    "                        adjust_touched.add(ij)\n",
    "        \n",
    "            # Plan further spread of the wave front\n",
    "            for rel_ij in [[-1, -1], [-1, 0], [-1, 1], [0, -1], [0, 1], [1, -1], [1, 0], [1, 1]]:\n",
    "                ij = int(wave_front_point[0] + rel_ij[0]), int(wave_front_point[1] + rel_ij[1])\n",
    "                # LOG(f'... spread, checking coord {rel_ij} {ij}')\n",
    "        \n",
    "                if ij[0] < 0 or ij[1] < 0 or ij[0] >= altitude_map.shape[0] or ij[1] >= altitude_map.shape[1]:\n",
    "                    # LOG(f'... spread, {ij} out of boundaries')\n",
    "                    continue\n",
    "                elif np.abs(altitude_map[ij]) >= 2: # coord_ij falls out of valley\n",
    "                    # LOG(f'... spread, {ij} has bad altitude {altitude_map[ij]}')\n",
    "                    continue\n",
    "                elif ij in wave_front_touched:\n",
    "                    # LOG(f'... spread, {ij} already touched ({len(wave_front_touched)})')\n",
    "                    continue\n",
    "        \n",
    "                wave_front_touched.add(ij)\n",
    "                wave_front_que.append(ij)\n",
    "                wave_front_map[ij] = wave_front_sensor_inst_ind\n",
    "                # LOG(f'... new wave front point {wave_front_que[-1]} (si={wave_front_sensor_inst_ind})')\n",
    "    \n",
    "        raw_sequences[start_sensor_inst_index] = sensor_inst_chains\n",
    "        # Collect all sensor instances encountered during this run and account them as sorted\n",
    "        sorted_sensor_instance_inds = set(itertools.chain.from_iterable(sensor_inst_chains.values()))\n",
    "        unsorted_sensor_instance_inds -= sorted_sensor_instance_inds\n",
    "\n",
    "    return raw_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd64343-8f42-43c7-bc57-46c39de883f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weave_raw_sequences(raw_sequences):\n",
    "    sequences = []\n",
    "        \n",
    "    for sensor_inst_chains in raw_sequences.values():\n",
    "        # Our goal is to find two longest subsequence which start from the same si and clue them together. \n",
    "        # Usually this would be two subsequences starting from start_sensor_inst_index.\n",
    "        # This way we would get longest serial sequence of traverse\n",
    "        subsequences = []\n",
    "        \n",
    "        for chain_index, chain in enumerate(sensor_inst_chains.values()):\n",
    "            chains_wo_chain_index = map(lambda v: v[1], filter(lambda v: v[0] != chain_index, enumerate(sensor_inst_chains.values())))\n",
    "            chains_dict = defaultdict(list)\n",
    "            for v in chains_wo_chain_index: chains_dict[v[0]].append(v)\n",
    "            \n",
    "            subsequences.append([])\n",
    "            branch_stack = [(chain, 0, len(subsequences) - 1)]\n",
    "            # LOG(f'Starting new subsequence #{len(subsequences) - 1}, {chain[0]}')\n",
    "        \n",
    "            while branch_stack:\n",
    "                chain, chain_elem_index, subseq_index = branch_stack.pop()\n",
    "                subseq = subsequences[subseq_index]\n",
    "                # LOG(f'Weaving subsequence #{subseq_index}, {chain[chain_elem_index]}')\n",
    "                \n",
    "                for i in range(chain_elem_index, len(chain)):\n",
    "                    v = chain[i]\n",
    "                    subseq.append(v)\n",
    "                    # LOG(f'... adding {chain[i]}, subseq len={len(subseq)}, subseq[:3]={subseq[:3]}, subseq[-3:]={subseq[-3:]}')\n",
    "        \n",
    "                    if v in chains_dict:\n",
    "                        # LOG(f'... branch point detected')\n",
    "                        branch_stack.append((chain, i + 1, subseq_index)) # continue sequence by following current chain\n",
    "    \n",
    "                        # Start new subsequences from new chain\n",
    "                        for branch_chain in chains_dict[v]:\n",
    "                            new_subseq = subseq.copy()\n",
    "                            subsequences.append(new_subseq)\n",
    "                            branch_stack.append((branch_chain, 1, len(subsequences) - 1)) # create new sequence and follow new chain\n",
    "                            # LOG(f'... branching new subsequence #{len(subsequences) - 1}, {branch_chain[1]}')\n",
    "                            \n",
    "                        break\n",
    "    \n",
    "        longest_sequence = []\n",
    "    \n",
    "        if not subsequences:\n",
    "            continue\n",
    "        elif len(subsequences) == 1:\n",
    "            longest_subsequence = subsequences[0]\n",
    "        else:\n",
    "            for ii in itertools.combinations(range(len(subsequences)), 2):\n",
    "                subseq1 = subsequences[ii[0]]\n",
    "                subseq2 = subsequences[ii[1]]\n",
    "                prefix_i = 0\n",
    "        \n",
    "                while prefix_i < min(len(subseq1), len(subseq2)) and subseq1[prefix_i] == subseq2[prefix_i]:\n",
    "                    prefix_i += 1\n",
    "        \n",
    "                if prefix_i == 0:\n",
    "                    # No common prefix, cant clue these two subseqs\n",
    "                    continue\n",
    "        \n",
    "                sequence = list(reversed(subseq1[prefix_i:])) + [subseq1[prefix_i - 1]] + subseq2[prefix_i:]\n",
    "        \n",
    "                if len(sequence) > len(longest_sequence):\n",
    "                    longest_sequence = sequence\n",
    "    \n",
    "        if longest_sequence:\n",
    "            sequences.append(longest_sequence)\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb595a-440e-401b-baaa-4051e78ec008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_sequences(sensor_instance_ind_dict, sequences, ma_window_size=4, precision=15):\n",
    "    def _moving_average(a, n=3):\n",
    "        ret = np.cumsum(a, dtype=float)\n",
    "        ret[n:] = ret[n:] - ret[:-n]\n",
    "        ret[:n-1] = np.array(a[:n-1]) * n\n",
    "        return ret / n\n",
    "\n",
    "    sequences_as_normal = []\n",
    "\n",
    "    for s in sequences:\n",
    "        sequence_as_normal = list(map(lambda si_ind: sensor_instance_ind_dict[si_ind].normal, s))\n",
    "        sequences_as_normal.append(sequence_as_normal)\n",
    "\n",
    "    sequences_simplified = []\n",
    "    \n",
    "    for sequence_as_normal, sequence in zip(sequences_as_normal, sequences):\n",
    "        sequence_as_normal_diffs = np.array(sequence_as_normal) - np.hstack([[sequence_as_normal[0]], sequence_as_normal[:-1]])\n",
    "        sequence_as_normal_diffs = (sequence_as_normal_diffs + 180) % 360 - 180\n",
    "        sequence_as_normal_diffs_ma = _moving_average(sequence_as_normal_diffs, ma_window_size)\n",
    "        \n",
    "        shuttle_map = np.zeros_like(sequence_as_normal_diffs_ma, dtype='i')\n",
    "        shuttles = dict()\n",
    "        current_normal_diff = None\n",
    "        insensitivity_lock = 0\n",
    "        \n",
    "        for i in range(ma_window_size, sequence_as_normal_diffs_ma.shape[0]):\n",
    "            if current_normal_diff is None or abs(current_normal_diff - sequence_as_normal_diffs_ma[i]) > precision / 2:\n",
    "                \n",
    "                if insensitivity_lock == 0:\n",
    "                    current_normal_diff = sequence_as_normal_diffs_ma[i]\n",
    "    \n",
    "                    if i > 0:\n",
    "                        assert shuttle_map[i-1] == 0\n",
    "                        shuttle_id = len(shuttles) + 1\n",
    "                        shuttles[shuttle_id] = (i - 1, -1)\n",
    "                        shuttle_map[i-1] = shuttle_id\n",
    "                        sequence_as_normal_diffs_ma[i-1] = sequence_as_normal_diffs_ma[i]\n",
    "    \n",
    "                    if i < shuttle_map.shape[0] - 1:\n",
    "                        assert shuttle_map[i+1] == 0\n",
    "                        shuttle_id = len(shuttles) + 1\n",
    "                        shuttles[shuttle_id] = (i + 1, +1)\n",
    "                        shuttle_map[i+1] = shuttle_id\n",
    "                        sequence_as_normal_diffs_ma[i+1] = sequence_as_normal_diffs_ma[i]\n",
    "                    \n",
    "                    insensitivity_lock += 3\n",
    "        \n",
    "            insensitivity_lock = max(0, insensitivity_lock - 1)\n",
    "\n",
    "        while shuttles:\n",
    "            dead_shuttles = []\n",
    "            \n",
    "            for shuttle_id, (shuttle_pos, shuttle_step) in shuttles.items():\n",
    "                assert shuttle_map[shuttle_pos] == shuttle_id\n",
    "                shuttle_map[shuttle_pos] = 0\n",
    "                new_pos = shuttle_pos + shuttle_step\n",
    "        \n",
    "                if new_pos < 0 or new_pos >= shuttle_map.shape[0]:\n",
    "                    dead_shuttles.append(shuttle_id)\n",
    "                elif shuttle_map[new_pos] != 0:\n",
    "                    rival_shuttle_id = shuttle_map[new_pos]\n",
    "                    assert shuttles[rival_shuttle_id][1] == -shuttle_step\n",
    "                    dead_shuttles.append(shuttle_id)\n",
    "                    dead_shuttles.append(rival_shuttle_id)\n",
    "                else:\n",
    "                    shuttle_map[new_pos] = shuttle_id\n",
    "                    shuttles[shuttle_id] = (new_pos, shuttle_step)\n",
    "                    sequence_as_normal_diffs_ma[new_pos] = sequence_as_normal_diffs_ma[shuttle_pos - shuttle_step]\n",
    "        \n",
    "            for shuttle_id in dead_shuttles:\n",
    "                del shuttles[shuttle_id]\n",
    "        \n",
    "        corner_indices = (1 + (np.argwhere(sequence_as_normal_diffs_ma[1:] != sequence_as_normal_diffs_ma[:-1]))).ravel()\n",
    "        sequence_simplified = []\n",
    "    \n",
    "        for i in itertools.chain.from_iterable([[0], corner_indices, [sequence_as_normal_diffs_ma.shape[0] - 1]]):\n",
    "            sequence_simplified.append(sequence[i])\n",
    "\n",
    "        if sequence_simplified:\n",
    "            sequences_simplified.append(sequence_simplified)\n",
    "\n",
    "    return sequences_simplified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
